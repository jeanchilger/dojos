{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento de Texto em Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem diversas etapas diferentes envolvendo o pré-processamento de um texto em python. A aplicação em que será empregado pode gerar variações na ordem destas etapas, bem como possibilitar que algumas sejam omitidas. Neste _notebook_ exploraremos uma alternativa para cumprir a tarefa de forma mais genérica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Remoção de tags HTML](#Remo%C3%A7%C3%A3o-de-tags-HTML)\n",
    "1. [Tratamento-para-Acentuação](#Tratamento-para-Acentua%C3%A7%C3%A3o)\n",
    "1. [Tokenização](#Tokeniza%C3%A7%C3%A3o)\n",
    "1. Remoção de _Stop Words_\n",
    "1. \n",
    "2. Lemmatização\n",
    "3. Outras etapas\n",
    "\n",
    "[Referências](#Refer%C3%AAncias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoção de tags HTML\n",
    "\n",
    "Muitos textos provém de _crawlers_ que acabam retornando muitos \"lixos\" junto com o material de interesse. Um destes \"lixos\" são tags HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove apenas as tags HTML (incluindo parâmetros), mantendo o texto contido nelas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def strip_html_tags(text, separator=\"\"):\n",
    "    parser = BeatifulSoup(text, \"html.parser\")\n",
    "    \n",
    "    return parser.get_text(separator=separator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove as tags e o conteúdo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def delete_html_nodes(text):\n",
    "    regex = re.compile(\"<.+>\")\n",
    "    \n",
    "    return re.sub(regex, \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento para Acentuação\n",
    "\n",
    "Caracteres acentuados como `à`, `ä` e `é` introduzem distinções (por exemplo `a` $\\neq$ `à` $\\neq$ `á`) indesejadas, que em etapas posteriores ao pré-processamento resultarão em problemas maiores.\n",
    "\n",
    "Para evitar isto, basta remover estas acentuações. Uma forma simples de fazê-lo é converter caracteres no formato unicode para o formato ascii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "def utf8_to_ascii(text):\n",
    "    return unidecode.unidecode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização\n",
    "\n",
    "[explicar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "\n",
    "def tokenize(corpus):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "    tokens = []\n",
    "    for document in corpus:\n",
    "        spacy_doc = nlp(document)\n",
    "        \n",
    "        tokens.append([token for token in spacy_doc])\n",
    "            \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematização e _Stemming_\n",
    "\n",
    "Os processos de lematização e de _stemming_ possuem o mesmo objetivo: reduzir formas flexionadas de linguagem (por exemplo verbos conjugados) para seu formato elementar. A diferença básica entre as técnicas é, sem muito rigor, que a lematização é um processo mais inteligente, capaz de encontrar os radicais mesmo para verbos irregulares (para o caso em que a palavra analisada é um verbo :)). Em geral, utilizar a lematização elimina a necessidade de _stemming_, e portanto utilizaremos apenas ela. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento para Números\n",
    "\n",
    "[explicar]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "[1] [All you need to know about text preprocessing for NLP and Machine Learning](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html);\n",
    "\n",
    "[2] [NLP Text Preprocessing: A Practical Guide and Template](https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79);\n",
    "\n",
    "[3] []();\n",
    "\n",
    "[4] []();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
